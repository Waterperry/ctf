FROM huggingface/transformers-pytorch-gpu

RUN mkdir /app
RUN useradd --create-home python
RUN chown -R python /app
USER python

RUN mkdir /home/python/.cache
RUN chown -R python /home/python/.cache
RUN chmod 777 -R /home/python/.cache

# do the expensive stuff before copying the code so it stays cached
RUN pip install --no-cache-dir accelerate==1.3.0 transformers==4.48.1 fastapi==0.115.6 uvicorn==0.34.0 pydantic==2.10.5 pydantic_core==2.27.2 requests==2.32.3 nltk==3.9.1

# in env as we use in script
# alternatives are Qwen/Qwen2.5-1.3B-Instruct  Qwen/Qwen2.5-3B-Instruct  Qwen/Qwen2.5-7B-Instruct
# or basically any other LLM on huggingface. note that the "jailbreak" prompts don't necessarily work for other modes.
ENV MODEL_NAME="Qwen/Qwen2.5-0.5B-Instruct"

RUN python3 -c 'import os; import transformers; _ = transformers.AutoModelForCausalLM.from_pretrained(os.environ["MODEL_NAME"]); _ = transformers.AutoTokenizer.from_pretrained(os.environ["MODEL_NAME"])'

COPY ./server.py /app
COPY ./common /app/common
COPY ./challenges /app/challenges

EXPOSE 8080

WORKDIR /app
CMD ["python3" ,"server.py"]
